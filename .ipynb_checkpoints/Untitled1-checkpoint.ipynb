{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asennuskomennot.txt\t       labels.txt\t scripts\r\n",
      "big2.sh\t\t\t       logs\t\t slurm-run.sh\r\n",
      "bigFileFinder.sh\t       models\t\t slurm-setup.sh\r\n",
      "Copy_of_debug_notebook.ipynb   preprocess.py\t tasks.py\r\n",
      "data\t\t\t       __pycache__\t Untitled1.ipynb\r\n",
      "debug_notebook_in_colab.ipynb  requirements.txt  utils_ner.py\r\n",
      "debug_notebook.ipynb\t       runs\t\t venv_transformers_\r\n",
      "jupyter.3536233.out\t       run_tf_ner.py\r\n",
      "jupyter.3634002.out\t       s800Data\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load gcc/8.3.0 cuda/10.1.168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=\"128\"\n",
    "BERT_MODEL=\"monologg/biobert_v1.0_pubmed_pmc\"\n",
    "BERT_MODEL=\"./biobert/biobert_v1.0_pubmed_pmc\"\n",
    "BERT_MODEL=\"monologg/biobert_v1.1_pubmed\"\n",
    "BERT_MODEL=\"./biobertTorch\"\n",
    "#bert_dir,\\\n",
    "#BERT_MODEL=\"./\"+bert_dir+\"/\"\n",
    "OUTPUT_DIR=\"s800_2\"\n",
    "BATCH_SIZE=\"32\"\n",
    "NUM_EPOCHS=\"4\"\n",
    "SAVE_STEPS=\"750\"\n",
    "SEED=\"1\"\n",
    "LEARNING_RATE=\"1e-5\"\n",
    "#  '--config_name', bert_model, \\\n",
    "      \n",
    "!rm -rf $OUTPUT_DIR    \n",
    "    \n",
    "argv = ['run_tf_ner.py','--data_dir','./data/s800SmallerData', \\\n",
    "        '--labels', './s800Data/labels.txt',\\\n",
    "        '--model_name_or_path', BERT_MODEL, \\\n",
    "        '--output_dir', OUTPUT_DIR, \\\n",
    "        '--max_seq_length',  MAX_LENGTH,\\\n",
    "        '--num_train_epochs', NUM_EPOCHS, \\\n",
    "        '--per_device_train_batch_size', BATCH_SIZE,\\\n",
    "        '--save_steps', SAVE_STEPS,\\\n",
    "        '--seed', SEED,\\\n",
    "        '--learning_rate', LEARNING_RATE,\\\n",
    "        '--do_train','--do_eval','--do_predict',\n",
    "        '--overwrite_output_dir']\n",
    "\n",
    "sys.argv = argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Fine-tuning the library models for named entity recognition.\"\"\"\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from importlib import import_module\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TFAutoModelForTokenClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments,\n",
    ")\n",
    "from utils_ner import Split, TFTokenClassificationDataset, TokenClassificationTask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    task_type: Optional[str] = field(\n",
    "        default=\"NER\", metadata={\"help\": \"Task type to fine tune in training (e.g. NER, POS, etc)\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n",
    "    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n",
    "    # or just modify its tokenizer_config.json.\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
    "    )\n",
    "    labels: Optional[str] = field(\n",
    "        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_2001426/harttu/july-2020/transformer/venv_transformers/lib/python3.7/site-packages/transformers/training_args.py:332: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# See all possible arguments in src/transformers/training_args.py\n",
    "# or by passing the --help flag to this script.\n",
    "# We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "    )\n",
    "\n",
    "module = import_module(\"tasks\")\n",
    "\n",
    "try:\n",
    "    token_classification_task_clazz = getattr(module, model_args.task_type)\n",
    "    token_classification_task: TokenClassificationTask = token_classification_task_clazz()\n",
    "except AttributeError:\n",
    "    raise ValueError(\n",
    "        f\"Task {model_args.task_type} needs to be defined as a TokenClassificationTask subclass in {module}. \"\n",
    "        f\"Available tasks classes are: {TokenClassificationTask.__subclasses__()}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFTrainingArguments(output_dir='s800_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct08_16-40-05_r02g07.bullx', logging_first_step=False, logging_steps=500, save_steps=750, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False, tpu_name=None, xla=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 16:40:21 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n",
      "10/08/2020 16:40:21 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='s800_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct08_16-40-05_r02g07.bullx', logging_first_step=False, logging_steps=500, save_steps=750, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False, tpu_name=None, xla=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(\n",
    "    \"n_replicas: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.n_replicas,\n",
    "    bool(training_args.n_replicas > 1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArguments(model_name_or_path='./biobertTorch', config_name=None, task_type='NER', tokenizer_name=None, use_fast=False, cache_dir=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available objects for config:\n",
      "     AliasManager\n",
      "     DisplayFormatter\n",
      "     HistoryManager\n",
      "     IPCompleter\n",
      "     IPKernelApp\n",
      "     LoggingMagics\n",
      "     MagicsManager\n",
      "     OSMagics\n",
      "     PrefilterManager\n",
      "     ScriptMagics\n",
      "     StoreMagics\n",
      "     ZMQInteractiveShell\n"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(\".bin\" in model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./biobertTorch'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER - GET_LABELS\n",
      "\tUSING:./s800Data/labels.txt\n",
      "\t\tRETURNING:['B-Species', 'B-Strain', 'I-Species', 'I-Strain', 'O']\n",
      "LABELS:\n",
      "['B-Species', 'B-Strain', 'I-Species', 'I-Strain', 'O']\n",
      "LABEL_MAP:\n",
      "{0: 'B-Species', 1: 'B-Strain', 2: 'I-Species', 3: 'I-Strain', 4: 'O'}\n",
      "CONFIG\n",
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"do_lower_case\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-Species\",\n",
      "    \"1\": \"B-Strain\",\n",
      "    \"2\": \"I-Species\",\n",
      "    \"3\": \"I-Strain\",\n",
      "    \"4\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-Species\": 0,\n",
      "    \"B-Strain\": 1,\n",
      "    \"I-Species\": 2,\n",
      "    \"I-Strain\": 3,\n",
      "    \"O\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Token Classification task\n",
    "labels = token_classification_task.get_labels(data_args.labels)\n",
    "print(\"LABELS:\")\n",
    "print(labels)\n",
    "label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
    "print(\"LABEL_MAP:\")\n",
    "print(label_map)\n",
    "num_labels = len(labels)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label=label_map,\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "print(\"CONFIG\")\n",
    "print(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast,\n",
    ")\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "    model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_pt=True,#bool(\".bin\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t   special_tokens_map.json  vocab.txt\r\n",
      "pytorch_model.bin  tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls $BERT_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING DATASETS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:15:22 - INFO - utils_ner -   Writing example 0 of 5740\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   guid: train-1\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   tokens: [CLS] Met ##han ##ore ##gu ##la form ##ici ##ca s ##p . no ##v . , a met ##hane - producing arch ##ae ##on isolated from met ##han ##ogenic s ##lu ##dge . [SEP]\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   input_ids: 101 19415 3822 4474 13830 1742 1532 27989 2599 188 1643 119 1185 1964 119 117 170 1899 15296 118 4411 9072 5024 1320 6841 1121 1899 3822 17960 188 7535 8484 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   label_ids: -100 0 -100 -100 -100 -100 2 -100 -100 4 -100 4 4 -100 4 4 4 4 -100 4 4 4 -100 -100 4 4 4 -100 -100 4 -100 -100 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   guid: train-2\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   tokens: [CLS] A novel met ##hane - producing arch ##ae ##on , strain SMS ##P ( T ) , was isolated from an an ##ae ##ro ##bic , prop ##ion ##ate - de ##grading en ##rich ##ment culture that was originally obtained from g ##ran ##ular s ##lu ##dge in a me ##so ##phi ##lic up ##flow an ##ae ##ro ##bic s ##lu ##dge blanket ( U ##AS ##B ) reactor used to treat a beer brewery e ##ff ##lue ##nt . [SEP]\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   input_ids: 101 138 2281 1899 15296 118 4411 9072 5024 1320 117 10512 25345 2101 113 157 114 117 1108 6841 1121 1126 1126 5024 2180 15421 117 21146 1988 2193 118 1260 20407 4035 10886 1880 2754 1115 1108 2034 3836 1121 176 4047 5552 188 7535 8484 1107 170 1143 7301 27008 8031 1146 12712 1126 5024 2180 15421 188 7535 8484 8560 113 158 10719 2064 114 15056 1215 1106 7299 170 5298 17876 174 3101 19224 2227 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   label_ids: -100 4 4 4 -100 4 4 4 -100 -100 4 4 1 -100 4 4 4 4 4 4 4 4 4 -100 -100 -100 4 4 -100 -100 4 4 -100 4 -100 -100 4 4 4 4 4 4 4 -100 -100 4 -100 -100 4 4 4 -100 -100 -100 4 -100 4 -100 -100 -100 4 -100 -100 4 4 4 -100 -100 4 4 4 4 4 4 4 4 4 -100 -100 -100 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   guid: train-3\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   tokens: [CLS] Cell ##s were non - m ##ot ##ile , blunt - ended , straight rods , 1 . 0 - 2 . 6 mum long by 0 . 5 mum wide ; cells were sometimes up to 7 mum long . [SEP]\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   input_ids: 101 17369 1116 1127 1664 118 182 3329 4759 117 20294 118 2207 117 2632 22598 117 122 119 121 118 123 119 127 23993 1263 1118 121 119 126 23993 2043 132 3652 1127 2121 1146 1106 128 23993 1263 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   label_ids: -100 4 -100 4 4 4 4 -100 -100 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   guid: train-4\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   tokens: [CLS] As ##ym ##metric ##al cell division was observed in rod - shaped cells . [SEP]\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   input_ids: 101 1249 17162 13689 1348 2765 2417 1108 4379 1107 14628 118 4283 3652 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   label_ids: -100 4 -100 -100 -100 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:15:22 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   guid: train-5\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   tokens: [CLS] Co ##cco ##id cells ( 0 . 5 - 1 . 0 mum in diameter ) were also observed in mid - to late - ex ##po ##nent ##ial phase cultures . [SEP]\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   input_ids: 101 3291 14566 2386 3652 113 121 119 126 118 122 119 121 23993 1107 6211 114 1127 1145 4379 1107 2286 118 1106 1523 118 4252 5674 21222 2916 4065 8708 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:22 - INFO - utils_ner -   label_ids: -100 4 -100 -100 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   Writing example 0 of 831\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   guid: dev-1\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   tokens: [CLS] F ##lav ##ob ##act ##eri ##um sin ##ops ##ych ##rot ##ole ##ran ##s s ##p . no ##v . , isolated from a glacier . [SEP]\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   input_ids: 101 143 9516 12809 11179 9866 1818 11850 9706 21155 10595 9016 4047 1116 188 1643 119 1185 1964 119 117 6841 1121 170 19121 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   label_ids: -100 0 -100 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 4 -100 4 4 -100 4 4 4 4 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   guid: dev-2\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   tokens: [CLS] An a ##ero ##bic , Gram - negative , yellow - pig ##mented bacterial strain , designated 05 ##33 ( T ) , was isolated from frozen soil from the China No . 1 glacier . [SEP]\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   input_ids: 101 1760 170 10771 15421 117 19891 118 4366 117 3431 118 13407 24674 19560 10512 117 3574 4991 23493 113 157 114 117 1108 6841 1121 7958 5384 1121 1103 1975 1302 119 122 19121 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   label_ids: -100 4 4 -100 -100 4 4 4 4 4 4 4 4 -100 4 4 4 4 1 -100 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   guid: dev-3\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   tokens: [CLS] Ph ##yl ##ogen ##etic analysis of the 16 ##S r ##RNA gene sequence demonstrated that strain 05 ##33 ( T ) was a member of the genus F ##lav ##ob ##act ##eri ##um and exhibited 97 . 1 - 98 . 7 % 16 ##S r ##RNA sequence similarity with its nearest phylogenetic neighbours . [SEP]\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   input_ids: 101 7642 7777 19790 9265 3622 1104 1103 1479 1708 187 15654 5565 4954 7160 1115 10512 4991 23493 113 157 114 1108 170 1420 1104 1103 2804 143 9516 12809 11179 9866 1818 1105 7799 5311 119 122 118 5103 119 128 110 1479 1708 187 15654 4954 15213 1114 1157 6830 28084 18832 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   label_ids: -100 4 -100 -100 -100 4 4 4 4 -100 4 -100 4 4 4 4 4 4 -100 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 -100 -100 4 4 4 4 4 4 4 4 4 4 4 -100 4 -100 4 4 4 4 4 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   guid: dev-4\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   tokens: [CLS] St ##rain 05 ##33 ( T ) exhibited p ##hen ##otypic and ch ##em ##ota ##xon ##omi ##c characteristics common to the genus F ##lav ##ob ##act ##eri ##um : men ##aq ##uin ##one - 6 ( M ##K - 6 ) was the predominant q ##uin ##one and is ##o - C ( 15 : 0 ) , C ( 17 : 1 ) o ##me ##ga ##6 ##c , ant ##ei ##so - C ( 15 : 0 ) , is ##o - C ( 15 : 0 ) 3 - OH , C ( 15 : 1 ) o ##me ##ga ##6 ##c , is ##o - C ( 16 : 0 ) 3 - OH , sum ##med feature 3 ( comprising [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:15:32 - INFO - utils_ner -   input_ids: 101 1457 11098 4991 23493 113 157 114 7799 185 10436 27202 1105 22572 5521 16339 21501 18882 1665 5924 1887 1106 1103 2804 143 9516 12809 11179 9866 1818 131 1441 22540 14846 4798 118 127 113 150 2428 118 127 114 1108 1103 23375 186 14846 4798 1105 1110 1186 118 140 113 1405 131 121 114 117 140 113 1542 131 122 114 184 3263 2571 1545 1665 117 22904 6851 7301 118 140 113 1405 131 121 114 117 1110 1186 118 140 113 1405 131 121 114 124 118 18719 117 140 113 1405 131 122 114 184 3263 2571 1545 1665 117 1110 1186 118 140 113 1479 131 121 114 124 118 18719 117 7584 4611 2672 124 113 9472 102\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   label_ids: -100 4 -100 1 -100 4 4 4 4 4 -100 -100 4 4 -100 -100 -100 -100 -100 4 4 4 4 4 4 -100 -100 -100 -100 -100 4 4 -100 -100 -100 4 4 4 4 -100 4 4 4 4 4 4 4 -100 -100 4 4 -100 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 -100 4 4 -100 -100 4 4 4 4 4 4 4 4 4 -100 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 -100 4 4 -100 4 4 4 4 4 4 4 4 4 4 4 4 -100 4 4 4 4 -100\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   guid: dev-5\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   tokens: [CLS] The DNA G + C content was 32 . 5 m ##ol % . [SEP]\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   input_ids: 101 1109 5394 144 116 140 3438 1108 2724 119 126 182 4063 110 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:15:32 - INFO - utils_ner -   label_ids: -100 4 4 4 4 4 4 4 4 4 4 4 -100 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    }
   ],
   "source": [
    "print(\"SETTING DATASETS\")\n",
    "# Get datasets\n",
    "train_dataset = (\n",
    "    TFTokenClassificationDataset(\n",
    "        token_classification_task=token_classification_task,\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.train,\n",
    "    )\n",
    "    if training_args.do_train\n",
    "    else None\n",
    ")\n",
    "eval_dataset = (\n",
    "    TFTokenClassificationDataset(\n",
    "        token_classification_task=token_classification_task,\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.dev,\n",
    "    )\n",
    "    if training_args.do_eval\n",
    "    else None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != -100:\n",
    "                out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "    #print(\"ALIGN_PREDICTIONS:\"+str(len(out_label_list)))\n",
    "    return preds_list, out_label_list\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFTrainingArguments(output_dir='s800_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct08_16-40-05_r02g07.bullx', logging_first_step=False, logging_steps=500, save_steps=750, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False, tpu_name=None, xla=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING TRAINER\n"
     ]
    }
   ],
   "source": [
    "print(\"INITIALIZING TRAINER\")\n",
    "initial_learning_rate=0.001\n",
    "decay_steps=10000\n",
    "# Initialize our Trainer\n",
    "trainer = TFTrainer(\n",
    "    model=model,\n",
    "    #optimizer=(tf.keras.optimizers.SGD(\n",
    "    #              learning_rate=learning_rate_fn),None),\n",
    "    #optimizers=(tf.keras.optimizers.Adam(\n",
    "    #learning_rate=initial_learning_rate, beta_1=0.9, \n",
    "    #beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    #name='Adam'),tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    #initial_learning_rate, decay_steps, \n",
    "    #end_learning_rate=0.0001, power=1.0,\n",
    "    #cycle=False, name=None\n",
    "    #)),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset.get_dataset() if train_dataset else None,\n",
    "    eval_dataset=eval_dataset.get_dataset() if eval_dataset else None,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': <transformers.modeling_tf_bert.TFBertForTokenClassification at 0x7f658899d208>,\n",
       " 'args': TFTrainingArguments(output_dir='s800_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct08_16-40-05_r02g07.bullx', logging_first_step=False, logging_steps=500, save_steps=750, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False, tpu_name=None, xla=False),\n",
       " 'train_dataset': <_AssertCardinalityDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>,\n",
       " 'eval_dataset': <_AssertCardinalityDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>,\n",
       " 'compute_metrics': <function __main__.compute_metrics(p: transformers.trainer_utils.EvalPrediction) -> Dict>,\n",
       " 'optimizer': None,\n",
       " 'lr_scheduler': None,\n",
       " 'gradient_accumulator': <transformers.optimization_tf.GradientAccumulator at 0x7f658899d198>,\n",
       " 'global_step': 0,\n",
       " 'epoch_logging': 0,\n",
       " 'tb_writer': <tensorflow.python.ops.summary_ops_v2.ResourceSummaryWriter at 0x7f658899d080>}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_2001426/harttu/july-2020/transformer/venv_transformers/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:493: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"TRAINING\")\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:19:18 - INFO - __main__ -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:19:22 - INFO - __main__ -   ***** Eval results *****\n",
      "10/08/2020 17:19:22 - INFO - __main__ -     eval_loss = 0.02843616558955266\n",
      "10/08/2020 17:19:22 - INFO - __main__ -     eval_precision = 0.8277404921700223\n",
      "10/08/2020 17:19:22 - INFO - __main__ -     eval_recall = 0.8447488584474886\n",
      "10/08/2020 17:19:22 - INFO - __main__ -     eval_f1 = 0.8361581920903954\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"EVALUATING\")\n",
    "# Evaluation\n",
    "results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    result = trainer.evaluate()\n",
    "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "\n",
    "        for key, value in result.items():\n",
    "            logger.info(\"  %s = %s\", key, value)\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        results.update(result)\n",
    "\n",
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:19:22 - INFO - utils_ner -   Writing example 0 of 1632\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   guid: test-1\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   tokens: [CLS] St ##re ##pt ##oc ##oc ##cus u ##rs ##oris s ##p . no ##v . , isolated from the oral ca ##vi ##ties of bears . [SEP]\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   input_ids: 101 1457 1874 6451 13335 13335 6697 190 1733 25758 188 1643 119 1185 1964 119 117 6841 1121 1103 9619 11019 5086 4338 1104 8807 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   label_ids: -100 0 -100 -100 -100 -100 -100 2 -100 -100 4 -100 4 4 -100 4 4 4 4 4 4 4 -100 -100 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   guid: test-2\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   tokens: [CLS] Three Gram - positive , cat ##ala ##se - negative , co ##cc ##us - shaped organisms were isolated from the oral ca ##vi ##ties of bears . [SEP]\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   input_ids: 101 2677 19891 118 3112 117 5855 5971 2217 118 4366 117 1884 19515 1361 118 4283 12023 1127 6841 1121 1103 9619 11019 5086 4338 1104 8807 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   label_ids: -100 4 4 4 4 4 4 -100 -100 4 4 4 4 -100 -100 4 4 4 4 4 4 4 4 4 -100 -100 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   guid: test-3\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   tokens: [CLS] The is ##olate ##s were tentatively identified as a s ##tre ##pt ##oc ##oc ##cal species based on the results of bio ##chemical tests . [SEP]\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_ids: 101 1109 1110 14995 1116 1127 22285 3626 1112 170 188 7877 6451 13335 13335 7867 1530 1359 1113 1103 2686 1104 25128 16710 5715 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   label_ids: -100 4 4 -100 -100 4 4 4 4 4 4 -100 -100 -100 -100 -100 4 4 4 4 4 4 4 -100 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   guid: test-4\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   tokens: [CLS] Comparative 16 ##S r ##RNA gene se ##quencing studies confirmed that the organisms were members of the genus St ##re ##pt ##oc ##oc ##cus , but they did not correspond to any recognized species of the genus . [SEP]\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_ids: 101 25741 1479 1708 187 15654 5565 14516 27276 2527 3659 1115 1103 12023 1127 1484 1104 1103 2804 1457 1874 6451 13335 13335 6697 117 1133 1152 1225 1136 18420 1106 1251 3037 1530 1104 1103 2804 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   label_ids: -100 4 4 -100 4 -100 4 4 -100 4 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 -100 -100 4 4 4 4 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   guid: test-5\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   tokens: [CLS] The nearest phylogenetic relative of the new is ##olate ##s was St ##re ##pt ##oc ##oc ##cus rat ##ti AT ##CC 1964 ##5 ( T ) ( 98 . 6 % ) , however , DNA - DNA hybrid ##ization analysis showed that the is ##olate ##s displayed less than 15 % DNA - DNA related ##ness with the type strain of S . rat ##ti . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_ids: 101 1109 6830 28084 5236 1104 1103 1207 1110 14995 1116 1108 1457 1874 6451 13335 13335 6697 11631 3121 13020 12096 2668 1571 113 157 114 113 5103 119 127 110 114 117 1649 117 5394 118 5394 9890 2734 3622 2799 1115 1103 1110 14995 1116 6361 1750 1190 1405 110 5394 118 5394 2272 1757 1114 1103 2076 10512 1104 156 119 11631 3121 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   label_ids: -100 4 4 4 4 4 4 4 4 -100 -100 4 0 -100 -100 -100 -100 -100 2 -100 1 -100 3 -100 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 -100 4 4 4 4 4 -100 -100 4 4 4 4 4 4 4 4 4 -100 4 4 4 4 4 0 2 2 -100 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:19:31 - INFO - __main__ -   \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Species       0.81      0.87      0.84       809\n",
      "      Strain       0.78      0.54      0.64       164\n",
      "\n",
      "   micro avg       0.80      0.81      0.81       973\n",
      "   macro avg       0.79      0.70      0.74       973\n",
      "weighted avg       0.80      0.81      0.80       973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PREDICTING\")\n",
    "if training_args.do_predict:\n",
    "    test_dataset = TFTokenClassificationDataset(\n",
    "        token_classification_task=token_classification_task,\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.test,\n",
    "    )\n",
    "\n",
    "    predictions, label_ids, metrics = trainer.predict(test_dataset.get_dataset())\n",
    "    preds_list, labels_list = align_predictions(predictions, label_ids)\n",
    "    report = classification_report(labels_list, preds_list)\n",
    "\n",
    "    logger.info(\"\\n%s\", report)\n",
    "\n",
    "    output_test_results_file = os.path.join(training_args.output_dir, \"test_results.txt\")\n",
    "\n",
    "    with open(output_test_results_file, \"w\") as writer:\n",
    "        writer.write(\"%s\\n\" % report)\n",
    "\n",
    "    # Save predictions\n",
    "    output_test_predictions_file = os.path.join(training_args.output_dir, \"test_predictions.txt\")\n",
    "\n",
    "    with open(output_test_predictions_file, \"w\") as writer:\n",
    "        with open(os.path.join(data_args.data_dir, \"test.txt\"), \"r\") as f:\n",
    "            example_id = 0\n",
    "\n",
    "            for line in f:\n",
    "                if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                    writer.write(line)\n",
    "\n",
    "                    if not preds_list[example_id]:\n",
    "                        example_id += 1\n",
    "                elif preds_list[example_id]:\n",
    "                    output_line = line.split()[0] + \" \" + preds_list[example_id].pop(0) + \"\\n\"\n",
    "\n",
    "                    writer.write(output_line)\n",
    "                else:\n",
    "                    logger.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streptococcus B-Species\r\n",
      "ursoris I-Species\r\n",
      "sp O\r\n",
      ". O\r\n",
      "nov O\r\n",
      ". O\r\n",
      ", O\r\n",
      "isolated O\r\n",
      "from O\r\n",
      "the O\r\n",
      "oral O\r\n",
      "cavities O\r\n",
      "of O\r\n",
      "bears O\r\n",
      ". O\r\n",
      "\r\n",
      "Three O\r\n",
      "Gram O\r\n",
      "- O\r\n",
      "positive O\r\n"
     ]
    }
   ],
   "source": [
    "tmp = os.path.join(data_args.data_dir, \"test.txt\")\n",
    "!head -20 $tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streptococcus B-Species\r\n",
      "ursoris I-Species\r\n",
      "sp O\r\n",
      ". O\r\n",
      "nov O\r\n",
      ". O\r\n",
      ", O\r\n",
      "isolated O\r\n",
      "from O\r\n",
      "the O\r\n",
      "oral O\r\n",
      "cavities O\r\n",
      "of O\r\n",
      "bears O\r\n",
      ". O\r\n",
      "\r\n",
      "Three O\r\n",
      "Gram O\r\n",
      "- O\r\n",
      "positive O\r\n"
     ]
    }
   ],
   "source": [
    "tmp = os.path.join(training_args.output_dir, \"test_predictions.txt\")\n",
    "!head -20 $tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_transformers_",
   "language": "python",
   "name": "venv_transformers_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
