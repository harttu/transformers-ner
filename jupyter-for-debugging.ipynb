{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ajettuja_komentoja\t       jupyter.3650374.out    requirements.txt\r\n",
      "asennuskomennot.txt\t       jupyter.3650398.out    runs\r\n",
      "big2.sh\t\t\t       jupyter.3650402.out    run_tf_ner.py\r\n",
      "bigFileFinder.sh\t       jupyter.3650902.out    s800Data\r\n",
      "Copy_of_debug_notebook.ipynb   jupyter.3651590.out    scripts\r\n",
      "data\t\t\t       jupyter.3651606.out    slurm-run-2.sh\r\n",
      "debug_notebook_in_colab.ipynb  jupyter.3651722.out    slurm-run.sh\r\n",
      "debug_notebook.ipynb\t       labels.txt\t      slurm-setup.sh\r\n",
      "jupyter.3536233.out\t       logs\t\t      tasks.py\r\n",
      "jupyter.3634002.out\t       models\t\t      Untitled1.ipynb\r\n",
      "jupyter.3634168.out\t       preprocess.py\t      utils_ner.py\r\n",
      "jupyter.3638342.out\t       puhti-installation.sh  venv_transformers_\r\n",
      "jupyter.3649979.out\t       __pycache__\r\n",
      "jupyter.3649983.out\t       README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_2001426/harttu/july-2020/transformers-ner/venv_transformers_/bin/python3\r\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load gcc/8.3.0 cuda/10.1.168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.txt  labels.txt  s800  s800SmallTrain  smaller  test.txt  train.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=\"128\"\n",
    "BERT_MODEL=\"monologg/biobert_v1.0_pubmed_pmc\"\n",
    "BERT_MODEL=\"./biobert/biobert_v1.0_pubmed_pmc\"\n",
    "BERT_MODEL=\"monologg/biobert_v1.1_pubmed\"\n",
    "BERT_MODEL=\"./models/biobertTorch/\"\n",
    "#bert_dir,\\\n",
    "#BERT_MODEL=\"./\"+bert_dir+\"/\"\n",
    "OUTPUT_DIR=\"s800_2\"\n",
    "BATCH_SIZE=\"32\"\n",
    "NUM_EPOCHS=\"1\"\n",
    "SAVE_STEPS=\"750\"\n",
    "SEED=\"1\"\n",
    "LEARNING_RATE=\"1e-5\"\n",
    "#  '--config_name', bert_model, \\\n",
    "      \n",
    "!rm -rf $OUTPUT_DIR    \n",
    "    \n",
    "argv = ['run_tf_ner.py','--data_dir','./data/s800SmallTrain/', \\\n",
    "        '--labels', './data/s800SmallTrain/labels.txt',\\\n",
    "        '--model_name_or_path', BERT_MODEL, \\\n",
    "        '--output_dir', OUTPUT_DIR, \\\n",
    "        '--max_seq_length',  MAX_LENGTH,\\\n",
    "        '--num_train_epochs', NUM_EPOCHS, \\\n",
    "        '--per_device_train_batch_size', BATCH_SIZE,\\\n",
    "        '--save_steps', SAVE_STEPS,\\\n",
    "        '--seed', SEED,\\\n",
    "        '--learning_rate', LEARNING_RATE,\\\n",
    "        '--do_train','--do_eval','--do_predict',\n",
    "        '--overwrite_output_dir']\n",
    "\n",
    "sys.argv = argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Fine-tuning the library models for named entity recognition.\"\"\"\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from importlib import import_module\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TFAutoModelForTokenClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments,\n",
    ")\n",
    "from utils_ner import Split, TFTokenClassificationDataset, TokenClassificationTask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    task_type: Optional[str] = field(\n",
    "        default=\"NER\", metadata={\"help\": \"Task type to fine tune in training (e.g. NER, POS, etc)\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n",
    "    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n",
    "    # or just modify its tokenizer_config.json.\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
    "    )\n",
    "    labels: Optional[str] = field(\n",
    "        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# See all possible arguments in src/transformers/training_args.py\n",
    "# or by passing the --help flag to this script.\n",
    "# We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "    )\n",
    "\n",
    "module = import_module(\"tasks\")\n",
    "\n",
    "try:\n",
    "    token_classification_task_clazz = getattr(module, model_args.task_type)\n",
    "    token_classification_task: TokenClassificationTask = token_classification_task_clazz()\n",
    "except AttributeError:\n",
    "    raise ValueError(\n",
    "        f\"Task {model_args.task_type} needs to be defined as a TokenClassificationTask subclass in {module}. \"\n",
    "        f\"Available tasks classes are: {TokenClassificationTask.__subclasses__()}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFTrainingArguments(output_dir='s800_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct13_13-25-33_r07c49.bullx', logging_first_step=False, logging_steps=500, save_steps=750, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False, tpu_name=None, xla=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2020 13:28:19 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n",
      "10/13/2020 13:28:19 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='s800_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct13_13-25-33_r07c49.bullx', logging_first_step=False, logging_steps=500, save_steps=750, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False, tpu_name=None, xla=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename='test.log',\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(\n",
    "    \"n_replicas: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.n_replicas,\n",
    "    bool(training_args.n_replicas > 1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER - GET_LABELS\n",
      "\tUSING:./data/s800SmallTrain/labels.txt\n",
      "\t\tRETURNING:['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGderiv', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-LOCpart', 'I-ORG', 'I-ORGderiv', 'I-ORGpart', 'I-OTH', 'I-OTHderiv', 'I-OTHpart', 'I-PER', 'I-PERderiv', 'I-PERpart', 'O']\n",
      "LABELS:\n",
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGderiv', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-LOCpart', 'I-ORG', 'I-ORGderiv', 'I-ORGpart', 'I-OTH', 'I-OTHderiv', 'I-OTHpart', 'I-PER', 'I-PERderiv', 'I-PERpart', 'O']\n",
      "LABEL_MAP:\n",
      "{0: 'B-LOC', 1: 'B-LOCderiv', 2: 'B-LOCpart', 3: 'B-ORG', 4: 'B-ORGderiv', 5: 'B-ORGpart', 6: 'B-OTH', 7: 'B-OTHderiv', 8: 'B-OTHpart', 9: 'B-PER', 10: 'B-PERderiv', 11: 'B-PERpart', 12: 'I-LOC', 13: 'I-LOCderiv', 14: 'I-LOCpart', 15: 'I-ORG', 16: 'I-ORGderiv', 17: 'I-ORGpart', 18: 'I-OTH', 19: 'I-OTHderiv', 20: 'I-OTHpart', 21: 'I-PER', 22: 'I-PERderiv', 23: 'I-PERpart', 24: 'O'}\n",
      "CONFIG\n",
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"do_lower_case\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-LOC\",\n",
      "    \"1\": \"B-LOCderiv\",\n",
      "    \"2\": \"B-LOCpart\",\n",
      "    \"3\": \"B-ORG\",\n",
      "    \"4\": \"B-ORGderiv\",\n",
      "    \"5\": \"B-ORGpart\",\n",
      "    \"6\": \"B-OTH\",\n",
      "    \"7\": \"B-OTHderiv\",\n",
      "    \"8\": \"B-OTHpart\",\n",
      "    \"9\": \"B-PER\",\n",
      "    \"10\": \"B-PERderiv\",\n",
      "    \"11\": \"B-PERpart\",\n",
      "    \"12\": \"I-LOC\",\n",
      "    \"13\": \"I-LOCderiv\",\n",
      "    \"14\": \"I-LOCpart\",\n",
      "    \"15\": \"I-ORG\",\n",
      "    \"16\": \"I-ORGderiv\",\n",
      "    \"17\": \"I-ORGpart\",\n",
      "    \"18\": \"I-OTH\",\n",
      "    \"19\": \"I-OTHderiv\",\n",
      "    \"20\": \"I-OTHpart\",\n",
      "    \"21\": \"I-PER\",\n",
      "    \"22\": \"I-PERderiv\",\n",
      "    \"23\": \"I-PERpart\",\n",
      "    \"24\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 0,\n",
      "    \"B-LOCderiv\": 1,\n",
      "    \"B-LOCpart\": 2,\n",
      "    \"B-ORG\": 3,\n",
      "    \"B-ORGderiv\": 4,\n",
      "    \"B-ORGpart\": 5,\n",
      "    \"B-OTH\": 6,\n",
      "    \"B-OTHderiv\": 7,\n",
      "    \"B-OTHpart\": 8,\n",
      "    \"B-PER\": 9,\n",
      "    \"B-PERderiv\": 10,\n",
      "    \"B-PERpart\": 11,\n",
      "    \"I-LOC\": 12,\n",
      "    \"I-LOCderiv\": 13,\n",
      "    \"I-LOCpart\": 14,\n",
      "    \"I-ORG\": 15,\n",
      "    \"I-ORGderiv\": 16,\n",
      "    \"I-ORGpart\": 17,\n",
      "    \"I-OTH\": 18,\n",
      "    \"I-OTHderiv\": 19,\n",
      "    \"I-OTHpart\": 20,\n",
      "    \"I-PER\": 21,\n",
      "    \"I-PERderiv\": 22,\n",
      "    \"I-PERpart\": 23,\n",
      "    \"O\": 24\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Token Classification task\n",
    "labels = token_classification_task.get_labels(data_args.labels)\n",
    "print(\"LABELS:\")\n",
    "print(labels)\n",
    "label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
    "print(\"LABEL_MAP:\")\n",
    "print(label_map)\n",
    "num_labels = len(labels)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label=label_map,\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "print(\"CONFIG\")\n",
    "print(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast,\n",
    ")\n",
    "#'monologg/biobert_v1.1_pubmed',#\n",
    "with training_args.strategy.scope():\n",
    "    model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_pt=True,#bool(\".bin\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2020 13:28:25 - INFO - utils_ner -   Writing example 0 of 1127\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   *** Example ***\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   guid: train-1\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   tokens: [CLS] Met ##han ##ore ##gu ##la B - Species form ##ici ##ca I - Species s ##p O . O no ##v O . O , O a O met ##hane O - O producing O arch ##ae ##on O isolated O from O met ##han ##ogenic O s ##lu ##dge O . O [SEP]\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   input_ids: 101 19415 3822 4474 13830 1742 139 118 11763 1532 27989 2599 146 118 11763 188 1643 152 119 152 1185 1964 152 119 152 117 152 170 152 1899 15296 152 118 152 4411 152 9072 5024 1320 152 6841 152 1121 152 1899 3822 17960 152 188 7535 8484 152 119 152 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   label_ids: -100 24 -100 -100 -100 -100 -100 -100 -100 24 -100 -100 -100 -100 -100 24 -100 -100 24 -100 24 -100 -100 24 -100 24 -100 24 -100 24 -100 -100 24 -100 24 -100 24 -100 -100 -100 24 -100 24 -100 24 -100 -100 -100 24 -100 -100 -100 24 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   *** Example ***\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   guid: train-2\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   tokens: [CLS] A O novel O met ##hane O - O producing O arch ##ae ##on O , O strain O SMS ##P B - St ##rain ( O T O ) O , O was O isolated O from O an O an ##ae ##ro ##bic O , O prop ##ion ##ate O - O de ##grading O en ##rich ##ment O culture O that O was O originally O obtained O from O g ##ran ##ular O s ##lu ##dge O in O a O me ##so ##phi ##lic O up ##flow O an ##ae ##ro ##bic O s ##lu ##dge O blanket O ( O U ##AS ##B O ) O reactor O used O to O treat O a O beer O brewery O [SEP]\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   input_ids: 101 138 152 2281 152 1899 15296 152 118 152 4411 152 9072 5024 1320 152 117 152 10512 152 25345 2101 139 118 1457 11098 113 152 157 152 114 152 117 152 1108 152 6841 152 1121 152 1126 152 1126 5024 2180 15421 152 117 152 21146 1988 2193 152 118 152 1260 20407 152 4035 10886 1880 152 2754 152 1115 152 1108 152 2034 152 3836 152 1121 152 176 4047 5552 152 188 7535 8484 152 1107 152 170 152 1143 7301 27008 8031 152 1146 12712 152 1126 5024 2180 15421 152 188 7535 8484 152 8560 152 113 152 158 10719 2064 152 114 152 15056 152 1215 152 1106 152 7299 152 170 152 5298 152 17876 152 102\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   label_ids: -100 24 -100 24 -100 24 -100 -100 24 -100 24 -100 24 -100 -100 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 24 -100 24 -100 -100 -100 24 -100 24 -100 -100 24 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 24 -100 -100 -100 24 -100 24 -100 24 -100 -100 -100 -100 24 -100 -100 24 -100 -100 -100 -100 24 -100 -100 -100 24 -100 24 -100 24 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   *** Example ***\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   guid: train-3\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   tokens: [CLS] Cell ##s O were O non O - O m ##ot ##ile O , O blunt O - O ended O , O straight O rods O , O 1 O . O 0 O - O 2 O . O 6 O mum O long O by O 0 O . O 5 O mum O wide O ; O cells O were O sometimes O up O to O 7 O mum O long O . O [SEP]\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   input_ids: 101 17369 1116 152 1127 152 1664 152 118 152 182 3329 4759 152 117 152 20294 152 118 152 2207 152 117 152 2632 152 22598 152 117 152 122 152 119 152 121 152 118 152 123 152 119 152 127 152 23993 152 1263 152 1118 152 121 152 119 152 126 152 23993 152 2043 152 132 152 3652 152 1127 152 2121 152 1146 152 1106 152 128 152 23993 152 1263 152 119 152 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   label_ids: -100 24 -100 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   *** Example ***\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   guid: train-4\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   tokens: [CLS] As ##ym ##metric ##al O cell O division O was O observed O in O rod O - O shaped O cells O . O [SEP]\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   input_ids: 101 1249 17162 13689 1348 152 2765 152 2417 152 1108 152 4379 152 1107 152 14628 152 118 152 4283 152 3652 152 119 152 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   label_ids: -100 24 -100 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2020 13:28:25 - INFO - utils_ner -   *** Example ***\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   guid: train-5\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   tokens: [CLS] Co ##cco ##id O cells O ( O 0 O . O 5 O - O 1 O . O 0 O mum O in O diameter O ) O were O also O observed O in O mid O - O to O late O - O ex ##po ##nent ##ial O phase O cultures O . O [SEP]\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   input_ids: 101 3291 14566 2386 152 3652 152 113 152 121 152 119 152 126 152 118 152 122 152 119 152 121 152 23993 152 1107 152 6211 152 114 152 1127 152 1145 152 4379 152 1107 152 2286 152 118 152 1106 152 1523 152 118 152 4252 5674 21222 2916 152 4065 152 8708 152 119 152 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:25 - INFO - utils_ner -   label_ids: -100 24 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING DATASETS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2020 13:28:28 - INFO - utils_ner -   Writing example 0 of 831\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   *** Example ***\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   guid: dev-1\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   tokens: [CLS] F ##lav ##ob ##act ##eri ##um B - Species sin ##ops ##ych ##rot ##ole ##ran ##s I - Species s ##p O . O no ##v O . O , O isolated O from O a O glacier O . O [SEP]\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   input_ids: 101 143 9516 12809 11179 9866 1818 139 118 11763 11850 9706 21155 10595 9016 4047 1116 146 118 11763 188 1643 152 119 152 1185 1964 152 119 152 117 152 6841 152 1121 152 170 152 19121 152 119 152 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   label_ids: -100 24 -100 -100 -100 -100 -100 -100 -100 -100 24 -100 -100 -100 -100 -100 -100 -100 -100 -100 24 -100 -100 24 -100 24 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   *** Example ***\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   guid: dev-2\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   tokens: [CLS] An O a ##ero ##bic O , O Gram O - O negative O , O yellow O - O pig ##mented O bacterial O strain O , O designated O 05 ##33 B - St ##rain ( O T O ) O , O was O isolated O from O frozen O soil O from O the O China O No O . O 1 O glacier O . O [SEP]\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   input_ids: 101 1760 152 170 10771 15421 152 117 152 19891 152 118 152 4366 152 117 152 3431 152 118 152 13407 24674 152 19560 152 10512 152 117 152 3574 152 4991 23493 139 118 1457 11098 113 152 157 152 114 152 117 152 1108 152 6841 152 1121 152 7958 152 5384 152 1121 152 1103 152 1975 152 1302 152 119 152 122 152 19121 152 119 152 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   label_ids: -100 24 -100 24 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   *** Example ***\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   guid: dev-3\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   tokens: [CLS] Ph ##yl ##ogen ##etic O analysis O of O the O 16 ##S O r ##RNA O gene O sequence O demonstrated O that O strain O 05 ##33 O ( O T O ) O was O a O member O of O the O genus O F ##lav ##ob ##act ##eri ##um O and O exhibited O 97 O . O 1 O - O 98 O . O 7 O % O 16 ##S O r ##RNA O sequence O similarity O with O its O nearest O phylogenetic O neighbours O . O [SEP]\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   input_ids: 101 7642 7777 19790 9265 152 3622 152 1104 152 1103 152 1479 1708 152 187 15654 152 5565 152 4954 152 7160 152 1115 152 10512 152 4991 23493 152 113 152 157 152 114 152 1108 152 170 152 1420 152 1104 152 1103 152 2804 152 143 9516 12809 11179 9866 1818 152 1105 152 7799 152 5311 152 119 152 122 152 118 152 5103 152 119 152 128 152 110 152 1479 1708 152 187 15654 152 4954 152 15213 152 1114 152 1157 152 6830 152 28084 152 18832 152 119 152 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   label_ids: -100 24 -100 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 -100 24 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 24 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   *** Example ***\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   guid: dev-4\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   tokens: [CLS] St ##rain O 05 ##33 B - St ##rain ( O T O ) O exhibited O p ##hen ##otypic O and O ch ##em ##ota ##xon ##omi ##c O characteristics O common O to O the O genus O F ##lav ##ob ##act ##eri ##um O : O men ##aq ##uin ##one O - O 6 O ( O M ##K O - O 6 O ) O was O the O predominant O q ##uin ##one O and O is ##o O - O C O ( O 15 O : O 0 O ) O , O C O ( O 17 O : O 1 O ) O o ##me ##ga ##6 ##c O , O ant ##ei ##so O - O [SEP]\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   input_ids: 101 1457 11098 152 4991 23493 139 118 1457 11098 113 152 157 152 114 152 7799 152 185 10436 27202 152 1105 152 22572 5521 16339 21501 18882 1665 152 5924 152 1887 152 1106 152 1103 152 2804 152 143 9516 12809 11179 9866 1818 152 131 152 1441 22540 14846 4798 152 118 152 127 152 113 152 150 2428 152 118 152 127 152 114 152 1108 152 1103 152 23375 152 186 14846 4798 152 1105 152 1110 1186 152 118 152 140 152 113 152 1405 152 131 152 121 152 114 152 117 152 140 152 113 152 1542 152 131 152 122 152 114 152 184 3263 2571 1545 1665 152 117 152 22904 6851 7301 152 118 152 102\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2020 13:28:28 - INFO - utils_ner -   label_ids: -100 24 -100 -100 24 -100 -100 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 24 -100 24 -100 -100 -100 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 -100 24 -100 24 -100 -100 -100 -100 24 -100 24 -100 24 -100 24 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 24 -100 24 -100 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 -100 -100 -100 24 -100 24 -100 -100 -100 24 -100 -100\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   *** Example ***\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   guid: dev-5\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   tokens: [CLS] The O DNA O G O + O C O content O was O 32 O . O 5 O m ##ol O % O . O [SEP]\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   input_ids: 101 1109 152 5394 152 144 152 116 152 140 152 3438 152 1108 152 2724 152 119 152 126 152 182 4063 152 110 152 119 152 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/13/2020 13:28:28 - INFO - utils_ner -   label_ids: -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 24 -100 -100 24 -100 24 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    }
   ],
   "source": [
    "print(\"SETTING DATASETS\")\n",
    "# Get datasets\n",
    "train_dataset = (\n",
    "    TFTokenClassificationDataset(\n",
    "        token_classification_task=token_classification_task,\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.train,\n",
    "    )\n",
    "    if training_args.do_train\n",
    "    else None\n",
    ")\n",
    "eval_dataset = (\n",
    "    TFTokenClassificationDataset(\n",
    "        token_classification_task=token_classification_task,\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.dev,\n",
    "    )\n",
    "    if training_args.do_eval\n",
    "    else None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != -100:\n",
    "                out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "    #print(\"ALIGN_PREDICTIONS:\"+str(len(out_label_list)))\n",
    "    return preds_list, out_label_list\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ajettuja_komentoja\t       jupyter.3650374.out    requirements.txt\r\n",
      "asennuskomennot.txt\t       jupyter.3650398.out    runs\r\n",
      "big2.sh\t\t\t       jupyter.3650402.out    run_tf_ner.py\r\n",
      "bigFileFinder.sh\t       jupyter.3650902.out    s800_2\r\n",
      "Copy_of_debug_notebook.ipynb   jupyter.3651590.out    s800Data\r\n",
      "data\t\t\t       jupyter.3651606.out    scripts\r\n",
      "debug_notebook_in_colab.ipynb  jupyter.3651722.out    slurm-run-2.sh\r\n",
      "debug_notebook.ipynb\t       labels.txt\t      slurm-run.sh\r\n",
      "jupyter.3536233.out\t       logs\t\t      slurm-setup.sh\r\n",
      "jupyter.3634002.out\t       models\t\t      tasks.py\r\n",
      "jupyter.3634168.out\t       preprocess.py\t      Untitled1.ipynb\r\n",
      "jupyter.3638342.out\t       puhti-installation.sh  utils_ner.py\r\n",
      "jupyter.3649979.out\t       __pycache__\t      venv_transformers_\r\n",
      "jupyter.3649983.out\t       README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFTrainingArguments(output_dir='s800_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct13_13-25-33_r07c49.bullx', logging_first_step=False, logging_steps=500, save_steps=750, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False, tpu_name=None, xla=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING TRAINER\n"
     ]
    }
   ],
   "source": [
    "print(\"INITIALIZING TRAINER\")\n",
    "initial_learning_rate=0.001\n",
    "decay_steps=10000\n",
    "# Initialize our Trainer\n",
    "trainer = TFTrainer(\n",
    "    model=model,\n",
    "    #optimizer=(tf.keras.optimizers.SGD(\n",
    "    #              learning_rate=learning_rate_fn),None),\n",
    "    #optimizers=(tf.keras.optimizers.Adam(\n",
    "    #learning_rate=initial_learning_rate, beta_1=0.9, \n",
    "    #beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    #name='Adam'),tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    #initial_learning_rate, decay_steps, \n",
    "    #end_learning_rate=0.0001, power=1.0,\n",
    "    #cycle=False, name=None\n",
    "    #)),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset.get_dataset() if train_dataset else None,\n",
    "    eval_dataset=eval_dataset.get_dataset() if eval_dataset else None,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': <transformers.modeling_tf_bert.TFBertForTokenClassification at 0x7f4a8b3c9f28>,\n",
       " 'args': TFTrainingArguments(output_dir='s800_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct13_13-06-39_r07c49.bullx', logging_first_step=False, logging_steps=500, save_steps=750, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False, tpu_name=None, xla=False),\n",
       " 'train_dataset': <_AssertCardinalityDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>,\n",
       " 'eval_dataset': <_AssertCardinalityDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>,\n",
       " 'compute_metrics': <function __main__.compute_metrics(p:transformers.trainer_utils.EvalPrediction) -> Dict>,\n",
       " 'optimizer': None,\n",
       " 'lr_scheduler': None,\n",
       " 'gradient_accumulator': <transformers.optimization_tf.GradientAccumulator at 0x7f4a8b39db38>,\n",
       " 'global_step': 0,\n",
       " 'epoch_logging': 0,\n",
       " 'tb_writer': <tensorflow.python.ops.summary_ops_v2.ResourceSummaryWriter at 0x7f4a8b39db00>}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_2001426/harttu/july-2020/transformers-ner/venv_transformers_/lib64/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:493: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"TRAINING\")\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:19:18 - INFO - __main__ -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:19:22 - INFO - __main__ -   ***** Eval results *****\n",
      "10/08/2020 17:19:22 - INFO - __main__ -     eval_loss = 0.02843616558955266\n",
      "10/08/2020 17:19:22 - INFO - __main__ -     eval_precision = 0.8277404921700223\n",
      "10/08/2020 17:19:22 - INFO - __main__ -     eval_recall = 0.8447488584474886\n",
      "10/08/2020 17:19:22 - INFO - __main__ -     eval_f1 = 0.8361581920903954\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"EVALUATING\")\n",
    "# Evaluation\n",
    "results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    result = trainer.evaluate()\n",
    "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "\n",
    "        for key, value in result.items():\n",
    "            logger.info(\"  %s = %s\", key, value)\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        results.update(result)\n",
    "\n",
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:19:22 - INFO - utils_ner -   Writing example 0 of 1632\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   guid: test-1\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   tokens: [CLS] St ##re ##pt ##oc ##oc ##cus u ##rs ##oris s ##p . no ##v . , isolated from the oral ca ##vi ##ties of bears . [SEP]\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   input_ids: 101 1457 1874 6451 13335 13335 6697 190 1733 25758 188 1643 119 1185 1964 119 117 6841 1121 1103 9619 11019 5086 4338 1104 8807 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   label_ids: -100 0 -100 -100 -100 -100 -100 2 -100 -100 4 -100 4 4 -100 4 4 4 4 4 4 4 -100 -100 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   guid: test-2\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   tokens: [CLS] Three Gram - positive , cat ##ala ##se - negative , co ##cc ##us - shaped organisms were isolated from the oral ca ##vi ##ties of bears . [SEP]\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   input_ids: 101 2677 19891 118 3112 117 5855 5971 2217 118 4366 117 1884 19515 1361 118 4283 12023 1127 6841 1121 1103 9619 11019 5086 4338 1104 8807 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:22 - INFO - utils_ner -   label_ids: -100 4 4 4 4 4 4 -100 -100 4 4 4 4 -100 -100 4 4 4 4 4 4 4 4 4 -100 -100 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   guid: test-3\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   tokens: [CLS] The is ##olate ##s were tentatively identified as a s ##tre ##pt ##oc ##oc ##cal species based on the results of bio ##chemical tests . [SEP]\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_ids: 101 1109 1110 14995 1116 1127 22285 3626 1112 170 188 7877 6451 13335 13335 7867 1530 1359 1113 1103 2686 1104 25128 16710 5715 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   label_ids: -100 4 4 -100 -100 4 4 4 4 4 4 -100 -100 -100 -100 -100 4 4 4 4 4 4 4 -100 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   guid: test-4\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   tokens: [CLS] Comparative 16 ##S r ##RNA gene se ##quencing studies confirmed that the organisms were members of the genus St ##re ##pt ##oc ##oc ##cus , but they did not correspond to any recognized species of the genus . [SEP]\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_ids: 101 25741 1479 1708 187 15654 5565 14516 27276 2527 3659 1115 1103 12023 1127 1484 1104 1103 2804 1457 1874 6451 13335 13335 6697 117 1133 1152 1225 1136 18420 1106 1251 3037 1530 1104 1103 2804 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   label_ids: -100 4 4 -100 4 -100 4 4 -100 4 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 -100 -100 4 4 4 4 4 4 4 4 4 4 4 4 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   *** Example ***\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   guid: test-5\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   tokens: [CLS] The nearest phylogenetic relative of the new is ##olate ##s was St ##re ##pt ##oc ##oc ##cus rat ##ti AT ##CC 1964 ##5 ( T ) ( 98 . 6 % ) , however , DNA - DNA hybrid ##ization analysis showed that the is ##olate ##s displayed less than 15 % DNA - DNA related ##ness with the type strain of S . rat ##ti . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_ids: 101 1109 6830 28084 5236 1104 1103 1207 1110 14995 1116 1108 1457 1874 6451 13335 13335 6697 11631 3121 13020 12096 2668 1571 113 157 114 113 5103 119 127 110 114 117 1649 117 5394 118 5394 9890 2734 3622 2799 1115 1103 1110 14995 1116 6361 1750 1190 1405 110 5394 118 5394 2272 1757 1114 1103 2076 10512 1104 156 119 11631 3121 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/08/2020 17:19:23 - INFO - utils_ner -   label_ids: -100 4 4 4 4 4 4 4 4 -100 -100 4 0 -100 -100 -100 -100 -100 2 -100 1 -100 3 -100 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 -100 4 4 4 4 4 -100 -100 4 4 4 4 4 4 4 4 4 -100 4 4 4 4 4 0 2 2 -100 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2020 17:19:31 - INFO - __main__ -   \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Species       0.81      0.87      0.84       809\n",
      "      Strain       0.78      0.54      0.64       164\n",
      "\n",
      "   micro avg       0.80      0.81      0.81       973\n",
      "   macro avg       0.79      0.70      0.74       973\n",
      "weighted avg       0.80      0.81      0.80       973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PREDICTING\")\n",
    "if training_args.do_predict:\n",
    "    test_dataset = TFTokenClassificationDataset(\n",
    "        token_classification_task=token_classification_task,\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.test,\n",
    "    )\n",
    "\n",
    "    predictions, label_ids, metrics = trainer.predict(test_dataset.get_dataset())\n",
    "    preds_list, labels_list = align_predictions(predictions, label_ids)\n",
    "    report = classification_report(labels_list, preds_list)\n",
    "\n",
    "    logger.info(\"\\n%s\", report)\n",
    "\n",
    "    output_test_results_file = os.path.join(training_args.output_dir, \"test_results.txt\")\n",
    "\n",
    "    with open(output_test_results_file, \"w\") as writer:\n",
    "        writer.write(\"%s\\n\" % report)\n",
    "\n",
    "    # Save predictions\n",
    "    output_test_predictions_file = os.path.join(training_args.output_dir, \"test_predictions.txt\")\n",
    "\n",
    "    with open(output_test_predictions_file, \"w\") as writer:\n",
    "        with open(os.path.join(data_args.data_dir, \"test.txt\"), \"r\") as f:\n",
    "            example_id = 0\n",
    "\n",
    "            for line in f:\n",
    "                if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                    writer.write(line)\n",
    "\n",
    "                    if not preds_list[example_id]:\n",
    "                        example_id += 1\n",
    "                elif preds_list[example_id]:\n",
    "                    output_line = line.split()[0] + \" \" + preds_list[example_id].pop(0) + \"\\n\"\n",
    "\n",
    "                    writer.write(output_line)\n",
    "                else:\n",
    "                    logger.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streptococcus B-Species\r\n",
      "ursoris I-Species\r\n",
      "sp O\r\n",
      ". O\r\n",
      "nov O\r\n",
      ". O\r\n",
      ", O\r\n",
      "isolated O\r\n",
      "from O\r\n",
      "the O\r\n",
      "oral O\r\n",
      "cavities O\r\n",
      "of O\r\n",
      "bears O\r\n",
      ". O\r\n",
      "\r\n",
      "Three O\r\n",
      "Gram O\r\n",
      "- O\r\n",
      "positive O\r\n"
     ]
    }
   ],
   "source": [
    "tmp = os.path.join(data_args.data_dir, \"test.txt\")\n",
    "!head -20 $tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streptococcus B-Species\r\n",
      "ursoris I-Species\r\n",
      "sp O\r\n",
      ". O\r\n",
      "nov O\r\n",
      ". O\r\n",
      ", O\r\n",
      "isolated O\r\n",
      "from O\r\n",
      "the O\r\n",
      "oral O\r\n",
      "cavities O\r\n",
      "of O\r\n",
      "bears O\r\n",
      ". O\r\n",
      "\r\n",
      "Three O\r\n",
      "Gram O\r\n",
      "- O\r\n",
      "positive O\r\n"
     ]
    }
   ],
   "source": [
    "tmp = os.path.join(training_args.output_dir, \"test_predictions.txt\")\n",
    "!head -20 $tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_transformers_",
   "language": "python",
   "name": "venv_transformers_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
